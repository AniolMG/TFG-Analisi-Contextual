LinkedIn Artículos Personas Learning Empleos Unirse ahora Inicia sesión ¿Que pintan las tarjetas gráficas en la Inteligencia Artificial? Ángel José G. Ángel José G. KAM en PNY Technologies Europe Fecha de publicación: 26 jun 2023 Seguir Las GPU han pasado de ser un básico procesador para envíar imágenes a una pantalla a convertirse en el motor de la IA y la investigación mundial. El mercado de chips para IA alcanzará los 263 mil millones de dólares en 2030, 25 veces mas que hoy, con Nvidia como líder absoluto y su acción disparada Lo confieso: hasta hace apenas cinco años desconocía por completo que las tarjetas gráficas, o unidades de procesamiento gráfico (GPU) se usaran en Inteligencia Artificial (IA) y que fueran la herramienta mas importante de su desarrollo. De toda la vida de Dios, el cerebro, dueño y señor del ordenador era la unidad de procesamiento central (CPU). Las gráficas, como su propio nombre indica, solo servían para transportar imágenes y gráficos al monitor del PC y, algunas de ellas, movían más rápido los juegos y con más calidad gráfica. Ahí me había quedado. Entonces, ¿ por qué en la IA no se utilizan CPU? ¿por qué las tarjetas gráficas son tan sumamente importantes en todos los procesos de IA y cómo es posible que la acción de Nvidia, el líder mundial en GPU, no pare de subir? La razón está en la gran capacidad de cálculo que tienen, dado que la IA no se basa en otra cosa más que en cálculos. Para dirigir y procesar imágenes, las GPU necesitan operar en paralelo, o sea simultáneamente en varias áreas a la vez. Sin embargo las CPU trabajan secuencialmente, no se pueden desviar de su camino. Para procesar la Big Data o resolver cálculos matemáticos complejos, se requiere un tipo de procesador que permita el cálculo en paralelo, usando a diestro y siniestro y a la vez todas los núcleos o áreas posibles. Cuantos más núcleos tenga la GPU, más capacidad de cálculo. Imaginemos que queremos procesar todos los datos volcados en una red social durante un tiempo concreto; estamos buscando que tipo de comida esta siendo tendencia y cual será, dentro de ese tipo, el plato mas solicitado el fin de semana y la ciudad donde mas se demanda o demandará. Miles y miles de datos y comentarios, con su origen, autor, lugar en el que se escribieron, etc. Y tras ellos, arranca una tarjeta gráfica que los irá procesando a la vez, usando algoritmos, buscando patrones y haciendo predicciones, recolectando y analizando datos de aquí y de allí, simultáneamente hasta ir dando en el clavo. La investigación médica se desarrolla exponencialmente gracias a la IA, mas en concreto con técnicas de ML, Machine Learning, que consisten en el aprendizaje automático por parte de la máquina gracias al uso de unas reglas preconcebidas a las que llamamos algoritmos. Un ejemplo práctico nos ayudará a entender mejor como se relacionan entre ellos y lo útiles que son el Big Data, el ML, los algoritmos, la IA y las tarjetas gráficas. En un centro de investigación médica estudiaron las enfermedades colaterales en casos de leucemia. Se usaron algoritmos de ML que buceaban sin parar en una enorme base de datos de enfermos buscando patrones que identificaran esas dolencias colaterales. Los algoritmos, usando el poder de cálculo de tarjetas gráficas, detectaron en tan solo siete segundos a los pacientes que las tenían y que enfermedad colateral concreta era con un porcentaje de acierto de mas del 90%. Es impresionante el número de industrias, aplicaciones y usos en las que las tarjetas gráficas son el motor de la investigación; es más, cuesta encontrar sectores que no hayan sido invadidos por ellas o estén en ello. Desde la regulación inteligente y centralizada del tráfico en las ciudades, hasta el control de accesos, la identificación online de personas con cámaras de seguridad, pasando por la criptominería y todo tipo de investigaciones médicas, farmacéuticas, I+D+I, desarrollo de producto en fabricantes e industrias. Se puede decir que una parte importante de la investigación mundial está ya, y lo estará aún más, sostenida por tarjetas gráficas. La consultora AMR asegura que el mercado de los chips dedicados a IA superará los 263.000 millones de dólares en 2030. Hoy es de 11.000 millones, 25 veces menos. Teniendo en cuenta que Nvidia monopoliza este mercado, con el 80% de la cuota actual mundial en IA, no es de extrañar que la acción de la compañía esté explotando al alza en los últimos tiempos como si no hubiera un mañana. Los AMD, Intel, Google o Amazon no vieron la oportunidad que tenían delante. Nvidia se adelantó y pronto se puso a desarrollar arquitecturas y plataformas de computación como CUDA, diseñadas específicamente para procesos de cálculo en paralelo. Compró también compañías especializadas como Mellanox, que le costó cerca de 7.000 millones de dólares, y se dedicaba a mejorar la interconexión de servidores y centros de datos, las venas de la IA. Para este tipo de usos, no sirve cualquier tipo de tarjeta; debe tener muchos núcleos y una memoria de video de alta capacidad. Pocas de las gráficas que usan los gamers, solo algunas de las más caras y de muy alta gama, pueden servir para usos reales en IA o ser compatibles con las plataformas y servidores donde se insertan las GPU. De hecho, Nvidia no permite el uso de gráficas gaming, denominadas Geforce, en entornos de cálculo o con usos intensivos ya que para ello dispone de determinados modelos dentro de Quadro y Tesla, sus gamas profesionales, preparados para soportar estos usos. OpenAI necesitó la nada despreciable cantidad de 20.000 unidades del modelo Tesla A100 para montar su famoso ChatGPT, por los que pagó unos 350 millones de dólares. Puede que necesite otras 30.000 unidades para la versión de pago del chat. Esto parece que no ha hecho mas que empezar. Recomendar Comentar Compartir 5 1 comentario Renzo Javier Cárdenas Yzaguirre IT Help Desk Specialist | Dell EMC Service Engineer 3 meses Gracias por la valiosa información. Recomendar Responder 1 reacción Para ver o add a comment, inicia sesión Más artículos de este autor Bitcoin, reserva de valor y desobediencia 25 ene 2024 Otros usuarios han visto Comprendiendo CPU y GPU: Su papel en la Inteligencia Artificial Aldo Verteramo 7 meses ¿Por qué las GPUs son tan importantes para la Inteligencia Artificial? Alejandro Mitaritonna 4 años LinkedIn © 2024 Acerca de Accesibilidad Condiciones de uso Política de privacidad Política de cookies Política de copyright Política de marca Controles de invitados Pautas comunitarias Idioma