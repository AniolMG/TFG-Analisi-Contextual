Tu dones ales al projecte - Micromecenatge Continguts Què és Ciència Oberta? Concurs Col·labora amb nosaltres Neurones Fregides Com entrenar un model de llenguatge extens en català amb en Joan Llop Palao 23 min de lectura Publicat per Equip RecerCA'T | des. 31, 2023 | RecerCA'T Presentem en Joan Llop Palao, investigador al Centre de Supercomputació de Barcelona (BSC), en aquesta sisena entrevista i darrer episodi de la primera temporada del pòdcast de Ciència Oberta, el RecerCA’T. Parlem d’intel·ligència artificial, de models de llenguatge extensos i sobre quines dificultats tenen llengües com el català per poder-ne entrenar models. Escolta’ns a Spotify, a iVoox o llegeix la transcripció de l’episodi que trobaràs més avall! Transcripció de l’episodi Víctor: La intel·ligència artificial és una disciplina que està transformant radicalment la manera com vivim. Treballem i ens relacionem amb la tecnologia. No és només una idea de ciència-ficció, és una realitat que influeix cada cop més en la nostra vida quotidiana. Des de les seves humils arrels fins als avenços més recents, la intel·ligència artificial ha experimentat una evolució sorprenent. En l’actualitat, a través de sofisticats algoritmes d’aprenentatge automàtic, les màquines poden aprendre de les dades i millorar el seu rendiment sense intervenció humana directa. No obstant això, aquesta ràpida evolució no està exempta de reptes i qüestions ètiques. A mesura que la intel·ligència artificial es converteix en part integrant de la nostra societat, sorgeixen preocupacions sobre la privacitat, la seguretat i l’impacte social. Com afecta a la intel·ligència artificial les nostres feines? Com interacciona amb la nostra privacitat? Com influeix en la presa de decisions importants? Aquesta petita introducció que heu d’escoltar ha estat escrita per ChatGPT. Segur que us sona, oi? El 30 de novembre de 2022, la intel·ligència artificial va sacsejar la quotidianitat de moltes persones. ChatGPT quedava a la disposició de qualsevol persona amb accés a internet. Va ser una sorpresa majúscula conversar amb aquesta eina i veure com generava un text aparentment original i no un simple copia i enganxa. Les possibilitats són enormes. Pots demanar que escrigui un poema d’un Chihuahua que viatja a Júpiter, que et recomani receptes de cuina del Nepal o que et dissenyi una ruta cultural pels pobles del Baix Montseny. A partir d’aquí, van proliferar noves eines que van més enllà del text i que es poden implementar per la generació d’imatges, per reconeixement de veu, per la creació musical i artística en general, o fins i tot pel diagnòstic mèdic. Tanmateix i com ella mateixa reconeix, la intel·ligència artificial ens planteja reptes importants, oi, Pau? Pau: Hola, Víctor. Sí, sí, com molt bé dius, la intel·ligència artificial ens porta una infinitat d’aplicacions i facilitats, però també ens pot portar molts maldecaps i amenaces, especialment en el camp lingüístic. Per això avui tenim amb nosaltres un especialista de la IA, de la intel·ligència artificial, i més concretament amb models de llenguatge, en Joan Llop Palao. En Joan és graduat en ciències de la computació per la Universitat Politècnica de Catalunya i va cursar un màster en intel·ligència artificial a la mateixa universitat. Ara treballa al Centre Supercomputació de Barcelona a la Unitat de Tecnologies del Llenguatge, desenvolupant models de llenguatge amb una especial atenció a llengües minoritzades i minoritàries. Benvingut, Joan. Joan: Hola, moltes gràcies per convidar-me. Víctor: Benvingut. Joan, per trencar el gel, quin és el teu llenguatge de programació favorit i per què? Joan: C++ és amb el que ho vaig fer tot. Ho he fet tot i avui no em deixen treballar perquè tots estem centrats en Python. C++. Pau: Ets una bèstia, eh? C++. S’ha utilitzat moltíssim el concepte d’intel·ligència artificial, però no tothom sap què és. De fet, fins i tot nosaltres mateixos, quan en parlem, cada vegada que ho expliquem ho fem d’una manera diferent. Tu com ho explicaries? Què és la intel·ligència artificial, Joan? Joan: Això no m’ho pregunto jo. A veure, no ho sé. D’acord. Si fem servir el terme que va fer servir, crec, als anys 50, en primer lloc, diria que… Pau: Queda molt lluny, eh? Joan: Ja, però és que realment no està molt clar què és. Ara tot té intel·ligència artificial, tot és intel·ligència artificial, què és exactament? Potser ho podríem definir com qualsevol programa que aprèn per si mateix donades unes dades, però llavors podem posar coses extremament simples a dins, fins a coses molt complexes. Però no està molt clar què és. Avui dia, potser o quasi segur que podríem associar al concepte de aprenentatge profund, tot i que la intel·ligència artificial és molt més àmplia. No ho sé, la línia no és clara. Jo tampoc sabria definir-la. Pau: Ostres, ens ho faràs molt complicat, eh? Víctor: Ara ho comentaves tu, Pau, però sembla que la intel·ligència artificial sigui d’ara, potser de l’any passat com a molt, però això ve de molt més enllà, no? Joan: De fet, un dels primers problemes que es van plantejar, no crec que fos el primer, però un dels primers va ser un professor de Stanford, li va proposar a un alumne a veure si podia treballar amb textos humans, la tasca no me’n recordo exactament quina li va posar, així com un treballet tonto i tal. Aquest treballet tonto es va convertir en el processament del llenguatge natural, que avui dia encara no està ni molt a prop d’estar resolt. La intel·ligència artificial ve de molt lluny, tan lluny com els ordinadors, des que hi ha ordinadors que volem que els ordinadors facin coses automàticament cada cop més autònomes, més automàtiques, i suposo que d’aquí surt tot plegat, tot i que, com dèieu, no està clar si un programa que llegeix tres inputs i treu dos outputs és intel·ligent artificial o no, tot està per veure. La definició exacta no us la sabria donar jo. Víctor: Llavors, segons el que has comentat, creus que la paraula intel·ligència és l’adient per a aquests tipus d’eines? És el que més ho defineix? Joan: Sempre és un problema de definició, sempre. Primer has de definir intel·ligència per veure si la podem fer servir per això o no. És molt atractiu el terme, suposo que ve més en aquest sentit de publicitat i d’allò que una altra cosa. Pau: Tu treballes amb intel·ligència artificial, però més concretament en allò que en català anomenem els models de llenguatge extensos, que són, en anglès, els Large Language Models, que és una paraula que t’he sentit dir ja moltes vegades. Abans de parlar de com els creeu o enteneu, vull saber què fan, què ens permet fer un model de llenguatge extens? Joan: Un model de llenguatge extens, per començar, no és una sola cosa, en són moltes. Es poden classificar, principalment, en el que diem encoders i el que diem decoders. Un encoder vindria a ser un model que t’agafa un text i et produeix una representació amb números que simbolitzen aquell text. Un decoder, a partir d’un text, genera la següent paraula. Aquí tens les dues grans vessant avui en dia. Previ a això, hi havia altres models i altres coses que també se li deien models de llenguatge, que eren extensos, com els que coneixeu avui. Jo diria que són aquests dos, principalment. Pau: A partir d’una paraula, pots donar-li una entrada d’informació que serien una sèrie de paraules i a partir d’això et pot predir quines venen després o quines podrien venir més tard. Joan: Únicament una, una cada moment. Pau: Clar, però al xat GPT, tu li fas una pregunta i pots preguntar com arribo al Centre de Supercomputació de Barcelona i què està passant allà dins. Joan: Si t’ho pogués explicar… Pau: Al final ens donen moltes paraules, no? No ens està donant una sola paraula. Joan: Sí, però això és com tot. Tu tens un sistema que està fet per predir d’esquerra a dreta, igual que com llegim. Va llegint fins a arribar a l’última paraula i aquí genera la següent. Pot triar diverses. Va construint així fins a arribar a un moment que sembla que té sentit. Hi ha diferents algoritmes per predir aquesta seqüència de paraules. Això tot assumint els models, estil decoder, estil basats en l’arquitectura GPT, que és el que es fa servir habitualment. La principal és aquesta. Vas predient paraula a paraula i no només predius una sola paraula. Tenim una llista de paraules, és molt curiós com es configuren en aquestes llistes, que s’anomena el vocabulari del tokenitzador. Són llistes de paraules. Són totes les possibles paraules o trossets de paraula. Durant tota aquesta llista, li assigno un numeret de quant probable que la següent paraula sigui la següent. Pau: A partir d’això et va construint les frases o paràgrafs. Joan: Exacte. En poses una o vàries i la poses una altra vegada a l’entrada del model i el model torna a predir la següent. Hi ha moltes maneres de fer això. Al final et construeix una cosa que, sorprenentment, té sentit. Pau: Hem parlat que hi ha uns models que són una sèrie d’algoritmes que ens hi referim com a intel·ligència artificial, i que ens generen uns textos. Joan: En realitat, un model no és un algoritme. Un model el podríem definir com una capsa. És l’objecte que et genera la següent paraula. En realitat, no sé si es pot definir com un algoritme. És complex. Jo diria que l’algoritme seria més… Un algoritme ho podríem pensar com una recepta. La recepta per fer truita de patates és un algoritme. La recepta per entrenar aquest model potser ho definiria més com un conjunt de peces enganxades. Com un objecte. Pau: En aquest cas, què necessiteu? Perquè aquest model aprengui a parlar, podríem dir, o escriure, què necessiteu? Joan: Exemples del que volem que faci. Si volem que generi la següent paraula, necessitem molts textos. Pau: D’on es treuen aquests textos? Quin tipus de textos són? Joan: No es podia fer el que es fa ara perquè no es tenien el volum de textos que es tenen en un format digital. Gràcies a internet, ara tenim grans quantitats de text. Estem parlant de moltíssimes quantitats de text. Intentem agafar el que vindria a ser tot internet. Tot aquest text es converteix en uns formats determinats que li pots passar al model perquè, per cada context, et predigui la següent paraula i el model va aprenent així. Pau: Es podria dir que en el fons és com un nen petit que… Joan: No. Si comencem a fer analogies d’aquest estil, ens perdem en l’antropomorfització. Quan converteixes en una cosa que no és, fa por una mica. Perquè després tenim molts problemes ètics al respecte. No, no, no són nens petits. Pau: Un dels interessos de la unitat de tecnologies de llenguatge del BSC és desenvolupar models de llenguatge per al català en el marc del projecte AINA. Ens ho pots explicar una miqueta? Joan: El projecte Aina té l’objectiu de crear infraestructura per al català. Infraestructura entesa com a infraestructura per a la tecnologia del llenguatge català, i més concretament com dades i models. La idea és que un parlant de català, l’objectiu seria que tingués tots els serveis que té una llengua majoritària, com podria ser el castellà o l’anglès. En el cas del català, ens trobem amb una falta de dades, recursos, models, dades de molts tipus, models de molts tipus diferents. Hem d’entendre que el que sembla que estem vivint és una revolució. S’ha inventat la màquina de vapor d’aquest segle, o almenys així ho entenc jo. A partir d’ara, ha sortit fa molt poc i està encara en procés de ser millorada, però d’aquí a deu o quinze anys potser han crescut un piló d’empreses que es dediquen a utilitzar tots aquests recursos. Si aquests recursos només estan en anglès, les empreses només es dedicaran a treballar per coses en anglès i l’anglès se’ns menjarà de mala manera. També tenim el castellà que està aquí mateix i el castellà és una llengua extremadament estesa i si tindrà els mateixos recursos que l’anglès, el català hauria de tenir exactament els mateixos recursos. Pau: Per tant, nosaltres com a catalanoparlants i consumidors i usuaris de serveis, el desenvolupament d’aquests projectes ens permetrà gaudir de tots aquests serveis en la nostra llengua o facilitarà que es pugui desenvolupar les eines necessàries per fer-ho.